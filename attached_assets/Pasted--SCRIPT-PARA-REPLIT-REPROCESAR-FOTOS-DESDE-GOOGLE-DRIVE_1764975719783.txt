üìÑ SCRIPT PARA REPLIT ‚Äì REPROCESAR FOTOS DESDE GOOGLE DRIVE

Objetivo:

Reprocesar todas las fotos antiguas ya importadas desde Google Drive, pero en alta calidad controlada.

Hacerlo en batches, con control de errores y sin reventar la memoria ni los l√≠mites de tiempo.

Dejar trazabilidad en la base de datos (qu√© est√° procesado, qu√© fall√≥, etc.).

1. Diagn√≥stico r√°pido de por qu√© se deten√≠a el script

üë®‚Äçüíª Acciones para Replit (investigaci√≥n):

Buscar el script anterior de importaci√≥n de fotos (nombre de archivo/proceso).

Revisar:

¬øProcesaba TODAS las fotos en un solo for gigante?

¬øSin l√≠mite de concurrencia (muchos downloads + resizes a la vez)?

¬øSin try/catch general ni reintentos?

¬øCorr√≠a en un entorno con timeout (ej: funci√≥n HTTP / job que se corta a X minutos)?

Confirmar si usa alguna librer√≠a tipo sharp, jimp o similar para procesar im√°genes.

Ver logs de cuando ‚Äúse detiene‚Äù:

¬øError por tama√±o de memoria?

¬øTimeout?

¬øRate limit de Google Drive?

¬øFalla alguna promesa y no se maneja, parando todo?

üîç Conclusi√≥n esperada (spoiler):

Procesar 48k im√°genes en un solo proceso lineal y pesado sin batches ni manejo de errores es receta ideal para que se ‚Äúmuera‚Äù por memoria, timeout o rate limits.

2. Dise√±o del nuevo flujo de reprocesamiento
2.1. Modelo / columnas en la base de datos

üë®‚Äçüíª Acci√≥n: agregar/confirmar columnas en la tabla de fotos (o donde guardes las im√°genes de cada unidad):

En tabla UnitPhoto (nombre a ajustar):

id

unitId (FK a Unidad)

driveFileId (string) ‚Üí id del archivo en Google Drive.

originalUrl (string) ‚Üí si se guard√≥ alguna vez.

processedUrl (string, nullable) ‚Üí URL de la foto ya reprocesada y subida al storage final.

processedAt (datetime, nullable)

width (int, nullable)

height (int, nullable)

sizeBytes (int, nullable)

status (string/enum):

pending (nunca procesada)

processing

processed

error

errorMessage (string/text, nullable)

retryCount (int, default 0)

Si ya existe algo parecido, adaptarlo. La idea es tener una forma de saber:

Qu√© falta procesar.

Qu√© se quebr√≥.

Cu√°nto pesa cada imagen.

3. Worker / script de reprocesamiento en batches
3.1. Configuraci√≥n base

üë®‚Äçüíª Acci√≥n: crear un script dedicado tipo scripts/reprocessPhotos.ts (Node) que:

NO est√© atado a una petici√≥n HTTP (para evitar timeouts).

Se pueda ejecutar como:

node scripts/reprocessPhotos.js


o una tarea de ‚ÄúBackground worker / Cron‚Äù dentro de Replit.

Use:

Google Drive API para descargar la foto por driveFileId.

sharp (recomendado) para resize/compress.

El storage que ya uses (S3 / Supabase / Cloudinary / etc.) para subir la versi√≥n final.

3.2. Par√°metros / constantes

En el script:

BATCH_SIZE = 50 (cu√°ntas fotos procesa en cada iteraci√≥n de la DB).

CONCURRENCY = 5 (m√°ximo n√∫mero de fotos procesadas en paralelo).

TARGET_MAX_WIDTH = 1600

TARGET_MAX_HEIGHT = 1600 (o 1900, pero con fit: 'inside' da igual, se mantiene proporci√≥n).

JPEG_QUALITY = 75 (luego se ajusta si queda muy pesado).

3.3. L√≥gica principal

Pseudo-c√≥digo para Replit:

async function main() {
  while (true) {
    // 1. Obtener un batch de fotos pendientes
    const photos = await db.unitPhoto.findMany({
      where: {
        status: { in: ['pending', 'error'] },
        retryCount: { lt: 5 },
      },
      take: BATCH_SIZE,
    });

    if (photos.length === 0) {
      console.log('No hay m√°s fotos pendientes. Fin.');
      break;
    }

    console.log(`Procesando batch de ${photos.length} fotos...`);

    // Marcar como "processing" para evitar que otro worker las tome
    await markAsProcessing(photos.map(p => p.id));

    // 2. Procesar en paralelo con l√≠mite de concurrencia
    await processWithConcurrency(photos, CONCURRENCY);

    // (opcional) Pausa corta para evitar rate limit de Drive
    await sleep(2000);
  }
}


processWithConcurrency:

async function processWithConcurrency(photos, concurrency) {
  const queue = [...photos];
  const workers: Promise<void>[] = [];

  for (let i = 0; i < concurrency; i++) {
    workers.push((async () => {
      while (queue.length > 0) {
        const photo = queue.shift();
        if (!photo) return;
        await processSinglePhoto(photo);
      }
    })());
  }

  await Promise.all(workers);
}


processSinglePhoto:

async function processSinglePhoto(photo) {
  try {
    // 1. Descargar desde Google Drive
    const buffer = await downloadFromGoogleDrive(photo.driveFileId);

    // 2. Reprocesar con sharp
    const sharp = require('sharp');
    const image = sharp(buffer);
    const metadata = await image.metadata();

    const resizedBuffer = await image
      .resize({
        width: TARGET_MAX_WIDTH,
        height: TARGET_MAX_HEIGHT,
        fit: 'inside',
        withoutEnlargement: true,
      })
      .jpeg({
        quality: JPEG_QUALITY,
        progressive: true,
        mozjpeg: true,
      })
      .toBuffer();

    // 3. Subir a storage final
    const url = await uploadToStorage(resizedBuffer, {
      // ruta/nombre basado en unitId + photoId, etc.
    });

    // 4. Guardar en DB
    await db.unitPhoto.update({
      where: { id: photo.id },
      data: {
        processedUrl: url,
        processedAt: new Date(),
        width: metadata.width,
        height: metadata.height,
        sizeBytes: resizedBuffer.length,
        status: 'processed',
        retryCount: { increment: 1 },
        errorMessage: null,
      },
    });

    console.log(`Foto ${photo.id} procesada OK (${resizedBuffer.length} bytes).`);
  } catch (err) {
    console.error(`Error procesando foto ${photo.id}`, err);
    await db.unitPhoto.update({
      where: { id: photo.id },
      data: {
        status: 'error',
        retryCount: { increment: 1 },
        errorMessage: String(err.message || err),
      },
    });
  }
}


Clave:

Si algo falla en una foto, el script no se detiene, solo marca esa en error y sigue con las dem√°s.

Se limita el n√∫mero de fotos procesadas simult√°neamente para no saturar memoria ni la API de Drive.

4. Integraci√≥n con los roles / secciones visibles

Aunque el reprocesamiento es ‚Äúbackend‚Äù, podemos dar visibilidad en la app:

4.1. Admin HomesApp (super admin)

üë®‚Äçüíª Acciones para Replit:

En el panel de Admin HomesApp ‚Üí secci√≥n tipo ‚ÄúMantenimiento del sistema‚Äù o ‚ÄúMedia‚Äù:

Mostrar:

Total fotos

Fotos processed

Fotos pending

Fotos error

Espacio total estimado (sumatoria de sizeBytes convertido a GB).

Permitir:

Bot√≥n ‚ÄúVer fotos con error‚Äù ‚Üí tabla con id, unidad, errorMessage.

Bot√≥n ‚ÄúReintentar fotos con error‚Äù ‚Üí setear status=pending para esas.

4.2. Admin de Agencia / Asesor

üë®‚Äçüíª Acciones para Replit:

En la vista de ‚ÄúUnidad‚Äù (donde ven fotos):

Si todas las fotos est√°n processed ‚Üí nada raro, solo se ve bien.

Si alguna est√° en pending o error:

Mostrar icono (ej: ‚ö†Ô∏è) indicando que la calidad a√∫n est√° en proceso o que hay error.

Pero sin romper la experiencia; es m√°s un detalle interno.

5. Control de espacio (5‚Äì7 GB)

üë®‚Äçüíª Acciones para Replit:

Despu√©s de tener el worker funcionando, tomar una muestra de, por ejemplo, 100‚Äì200 fotos procesadas:

Sacar promedio de sizeBytes.

Estimar:

avgSizeBytes √ó 48000 y convertir a GB.

Si se pasa mucho de 7 GB:

Bajar JPEG_QUALITY de 75 ‚Üí 70 o 65.

Volver a procesar solo las m√°s grandes (o todas, seg√∫n lo que se decida).

Crear un peque√±o endpoint/admin interno:

GET /api/admin/storage-summary

Devuelve:

totalPhotos

totalBytes

avgBytes

Para que Admin HomesApp lo vea en su panel.

6. Ejecuci√≥n en Replit (muy importante)

üë®‚Äçüíª Acciones concretas:

Crear un Background Worker / Tarea programada (si el entorno lo permite) que ejecute node scripts/reprocessPhotos.js.

O al menos:

Permitir ejecutarlo manualmente desde la consola cuando se haga el reprocesado masivo.

Asegurarse de que:

El script loggee progreso (cada X fotos).

Se puedan ver logs si algo falla.