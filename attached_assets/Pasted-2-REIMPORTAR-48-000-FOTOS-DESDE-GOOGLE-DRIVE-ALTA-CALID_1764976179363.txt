2. REIMPORTAR 48,000 FOTOS DESDE GOOGLE DRIVE (ALTA CALIDAD)
2.1. Objetivo de tamaÃ±o y resoluciÃ³n

ðŸ‘‰ AcciÃ³n (config imagen):

Para cada foto:

Redimensionar con:

lado mÃ¡s largo â‰ˆ 1600 px (ej. width=1600 y height proporcional, sin deformar).

No aumentar tamaÃ±o si el original es menor (no hacer upscale).

Formato:

Ideal: WebP o JPEG optimizado.

TamaÃ±o objetivo promedio:

48,000 fotos â†’

100 KB c/u â†’ ~4.6 GB

120 KB c/u â†’ ~5.5 GB

150 KB c/u â†’ ~6.9 GB

ðŸ‘‰ Apuntar a 120â€“150 KB promedio por foto (ajustando quality).

2.2. Campos necesarios para migrar

ðŸ‘‰ AcciÃ³n (ver DB y ajustar modelo):

Asegurarse de que en cada Photo tengamos:

driveFileId o algÃºn identificador para volver a descargar el original desde Drive.

qualityVersion (0/1 vieja, 2 nueva).

migrationStatus (pending, processing, done, error).

Si no existe driveFileId, recordar de dÃ³nde se obtienen las fotos originalmente (carpeta en Drive, patrÃ³n de nombre, etc.) y guardar esa referencia antes de iniciar la migraciÃ³n.

2.3. Arquitectura del proceso de migraciÃ³n (para que NO se cuelgue)

ðŸ‘‰ AcciÃ³n (backend):

Crear una tarea de migraciÃ³n en background, NO dentro de una sola llamada HTTP larga.

Piezas:

Tabla/campo migrationStatus en Photo.

Script/worker tipo node scripts/migratePhotosHighRes.ts.

Procesamiento en lotes:

Ej.: 100â€“200 fotos por ciclo mÃ¡ximo.

LÃ­mites:

Concurrencia bajita (2â€“3 fotos en paralelo).

Retries controlados.

Flujo del worker:

Paso 1: Seleccionar N fotos con:

migrationStatus = 'pending'

y source = 'drive'

Paso 2: Marcar cada una como processing.

Paso 3: Para cada foto:

Descargar original desde Google Drive usando driveFileId.

Procesar imagen (resize + compress).

Subir a storage (el mismo que estÃ¡n usando ahora).

Actualizar url, width, height, sizeBytes, qualityVersion = 2, migrationStatus = 'done'.

(Opcional) Borrar archivo antiguo de baja calidad del storage.

Paso 4: Si hay error en alguna foto:

Marcar migrationStatus = 'error' y guardar errorMessage (nuevo campo opcional).

Paso 5: Salir cuando ya no haya mÃ¡s fotos pending.

EjecuciÃ³n:

Comando ejecutable en Replit:

node scripts/migratePhotosHighRes.js


Este script puede correr varias veces; cada vez procesarÃ¡ el siguiente lote de pending.

2.4. Ejemplo de pseudocÃ³digo del script de migraciÃ³n

ðŸ‘‰ AcciÃ³n (para Replit, Node/TS):

// scripts/migratePhotosHighRes.ts
import { getPendingPhotos, markAsProcessing, updatePhotoSuccess, markAsError } from '../db/photos';
import { downloadFromDrive } from '../services/googleDrive';
import { uploadToStorage } from '../services/storage';
import sharp from 'sharp';

const BATCH_SIZE = 100;     // fotos por corrida
const CONCURRENCY = 3;      // fotos en paralelo

async function processPhoto(photo) {
  try {
    // 1. Descargar desde Drive
    const originalBuffer = await downloadFromDrive(photo.driveFileId);

    // 2. Procesar con Sharp (o similar)
    const processed = await sharp(originalBuffer)
      .resize({
        width: 1600,
        withoutEnlargement: true,
      })
      .webp({ quality: 70 }) // ajustar calidad segÃºn pruebas
      .toBuffer();

    // 3. Subir a storage (ej: units/{unitId}/primary/{photoId}.webp)
    const path = `units/${photo.unitId}/${photo.type}/${photo.id}.webp`;
    const url = await uploadToStorage(path, processed, 'image/webp');

    // 4. Actualizar DB
    const metadata = await sharp(processed).metadata();
    await updatePhotoSuccess(photo.id, {
      url,
      width: metadata.width,
      height: metadata.height,
      sizeBytes: processed.length,
      qualityVersion: 2,
    });

  } catch (err) {
    console.error('Error processing photo', photo.id, err);
    await markAsError(photo.id, err.message || 'unknown error');
  }
}

async function main() {
  const photos = await getPendingPhotos(BATCH_SIZE);
  if (!photos.length) {
    console.log('No pending photos');
    return;
  }

  // marcar processing
  await markAsProcessing(photos.map(p => p.id));

  // procesar con concurrencia limitada
  const chunks = [];
  for (let i = 0; i < photos.length; i += CONCURRENCY) {
    chunks.push(photos.slice(i, i + CONCURRENCY));
  }

  for (const chunk of chunks) {
    await Promise.all(chunk.map(processPhoto));
  }

  console.log('Batch finished:', photos.length, 'photos processed');
}

main().then(() => process.exit(0));


importante:

downloadFromDrive debe usar la Google Drive API con rate limiting.

uploadToStorage puede ser el mismo servicio que ya usan hoy.

Probar primero con 100â€“200 fotos antes de lanzar todas las 48,000.